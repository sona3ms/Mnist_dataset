{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+NuE2dgMJhBySv3Ngxk+W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sona3ms/Mnist_dataset/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfWDUXWqYJwm",
        "outputId": "f4609a77-d6b2-4ede-cf44-f63bc633244f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 41s 64ms/step - loss: 0.6157 - acc: 0.6336 - val_loss: 0.4422 - val_acc: 0.8022\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 0.3767 - acc: 0.8367 - val_loss: 0.3764 - val_acc: 0.8500\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 0.3011 - acc: 0.8790 - val_loss: 0.4226 - val_acc: 0.8188\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 0.2507 - acc: 0.9035 - val_loss: 0.4083 - val_acc: 0.8344\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 0.2053 - acc: 0.9232 - val_loss: 0.4114 - val_acc: 0.8564\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 40s 63ms/step - loss: 0.1545 - acc: 0.9449 - val_loss: 0.4225 - val_acc: 0.8524\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 0.1126 - acc: 0.9585 - val_loss: 0.5306 - val_acc: 0.8168\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 39s 63ms/step - loss: 0.0827 - acc: 0.9720 - val_loss: 0.6217 - val_acc: 0.8158\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.0589 - acc: 0.9805 - val_loss: 0.7427 - val_acc: 0.7702\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 0.0430 - acc: 0.9855 - val_loss: 0.6967 - val_acc: 0.7994\n",
            "782/782 [==============================] - 11s 14ms/step - loss: 0.7266 - acc: 0.7933\n",
            "Test loss: 0.72660231590271, Test accuracy: 0.7933200001716614\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# Load the IMDB dataset\n",
        "max_features = 10000  # Only consider the top 10,000 words in the dataset\n",
        "max_len = 500  # Cut reviews after 500 words\n",
        "batch_size = 32\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Build the RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "results = model.evaluate(x_test, y_test)\n",
        "print(f'Test loss: {results[0]}, Test accuracy: {results[1]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleRNN:\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Weight matrices\n",
        "        self.Wx = np.random.randn(hidden_size, input_size)\n",
        "        self.Wh = np.random.randn(hidden_size, hidden_size)\n",
        "        self.Wy = np.random.randn(1, hidden_size)\n",
        "\n",
        "        # Bias vectors\n",
        "        self.bh = np.zeros((hidden_size, 1))\n",
        "        self.by = np.zeros((1, 1))\n",
        "\n",
        "        # Memory of previous hidden state\n",
        "        self.h_prev = np.zeros((hidden_size, 1))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass\n",
        "        self.x = x\n",
        "        self.a = np.dot(self.Wx, x) + np.dot(self.Wh, self.h_prev) + self.bh\n",
        "        self.h = np.tanh(self.a)\n",
        "        self.y = self.sigmoid(np.dot(self.Wy, self.h) + self.by)\n",
        "\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, dy):\n",
        "        # Backward pass\n",
        "        dh = np.dot(self.Wy.T, dy) * (1 - np.tanh(self.a)**2)\n",
        "        dWy = np.dot(dy, self.h.T)\n",
        "        dWh = np.dot(dh, self.h_prev.T)\n",
        "        dWx = np.dot(dh, self.x.T)\n",
        "        dbh = dh\n",
        "        dby = dy\n",
        "\n",
        "        # Update parameters using gradient descent\n",
        "        self.Wy -= dWy\n",
        "        self.Wh -= dWh\n",
        "        self.Wx -= dWx\n",
        "        self.bh -= dbh\n",
        "        self.by -= dby\n",
        "\n",
        "        # Gradient to be passed to the previous time step\n",
        "        dh_prev = np.dot(self.Wh.T, dh)\n",
        "\n",
        "        return dh_prev\n",
        "\n",
        "# Example usage\n",
        "input_size = 3\n",
        "hidden_size = 2\n",
        "rnn = SimpleRNN(input_size, hidden_size)\n",
        "\n",
        "# Input sequence\n",
        "sequence = [np.array([[1], [2], [3]]) for _ in range(5)]\n",
        "\n",
        "for step in sequence:\n",
        "    output = rnn.forward(step)\n",
        "    print(\"Output:\", output)\n",
        "\n",
        "# Backward pass (assuming a binary classification task)\n",
        "dy = np.array([[1]])\n",
        "rnn.backward(dy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL_HRiMUYN5o",
        "outputId": "39ae49a1-f3a6-44c9-b2e9-4c6d8feff7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: [[0.23654986]]\n",
            "Output: [[0.23654986]]\n",
            "Output: [[0.23654986]]\n",
            "Output: [[0.23654986]]\n",
            "Output: [[0.23654986]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.03672622],\n",
              "       [-0.01077504]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZMII9DAuZLUv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}